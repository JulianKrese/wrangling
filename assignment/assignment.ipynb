{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The framework for a Tidy Dataset ie. a dataset where each variable is a column, each observation is a row, and each observational unit is a table.\n",
        "2. Making data analysis easier by having a standard structure. Also makes data cleaning quicker because there is a standard model.\n",
        "3. Tidy Datasets are all similar to each other and thus can all be analyzed the same way. However, each messy dataset is unique and has to be processed in a unique way. - Variables and observations don't have a standard to describe them and as such aren't consistent with how they are used. As such, when viewing a dat set you can usually figure out which is which but they aren't standardized.\n",
        "4. Values = numbers or strings belonging to a variable or observation. Variables = values that measure the same underlying attribute across units (ex: height). Observations = values measured on the same unit across attributes (ex: a person).\n",
        "5. A dataset where each variable is a column, each observation is a row, and each observational unit is a table.\n",
        "6. (1) Column headers are values, not variable names (2) Multiple variables in one column (3) Variables stored in both rows and columns (4) multiple types of observational units are stored in the same table (5) A single observational unit is stored in multiple tables. - The data in Table 4 is messy because variables form the columns (income). - Melting datasets is turning columns into rows.\n",
        "7. In Table 11, variables (max and min) are in rows instead of columns - Table 12b properly reflects this change and creates a column for the variable date instead of having dates as columns.\n",
        "8. The chicken and the egg problems refers to which came first. Here, it translates to tidy data and analyzing tools. We need either our tools to work with our datasets, or our datasets to work with our tools. This means we work independently on one (dataset or tools) depending on which one \"came first\" so one is always limited by the other. Wickham hopes that tidy data is an early initiative to efficient data storage and tools that are better at processing data."
      ],
      "metadata": {
        "id": "r9lbV0ughEsI"
      },
      "id": "r9lbV0ughEsI"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link —> https://www.census.gov/library/stories/2021/08/improved-race-ethnicity-measures-reveal-united-states-population-much-more-multiracial.html#:~:text=The%202020%20Census%20used%20the,Budget%20(OMB)%20in%201997.\n",
        "1. The 2020 US Census gathered data with two questions; one for Hispanic or Latino origin and one for race.\n",
        "2. We gather data such as race so we can get an accurate picture of who Americans actually are. In politics and society, many viewpoints and policies are linked to demographics so race would be one important demographic that influences these viewpoints and policies. As such, having accurate data that is the basis for serious decisions is important for accurate and beneficial policies.\n",
        "3. The article said the two question approach has been incredibly helpful for getting more accurate data, but it also says a one question approach would ultimately be even more effective at producing accurate data. So the two question approach was helpful, but still a point of improvement.\n",
        "\n",
        "Link —> https://www.census.gov/library/stories/2021/11/census-bureau-survey-explores-sexual-orientation-and-gender-identity.html\n",
        "4. Gender Identity was measured with two questions, one asking for sex assigned at birth (M or F) and one asking how the responder now defines themselves (M, F, transgender, or none of these). Sexual Orientation was measured with one question asking how the responder best thinks of themselves, with 5 options below for gay or lesbian, bisexual, straight, something else, or don’t know. One bad practice is not giving the opportunity to decline to answer for sex assigned at birth and current self identification.\n",
        "5. I think one concern would be losing control of sensitive data. We need a way to confirm the people filling out the survey are real people, which will require some form of identification (or maybe not, but I feel there must be something linking people back to the answers they gave one way or another). And so, if private or sensitive data such as sexual orientation was leaked, that would be a huge problem and make US Citizens less likely to give honest answers in future surveys as there is no confidentiality. Another concern would be missing values. You don’t want to skew the data by including missing values that then under or over represent groups, but you also don’t want to throw away the people with missing data as their data is important. I don’t know what practices people might adopt from cleaning census data.\n",
        "6. First and foremost it would be wrong to label people with categories they specifically avoiding putting themselves into. There is a reason they declined to answer and that right to decline is being infringed upon by imputing it for them, along with losing the data that we get from them not answering. There is a reason they didn’t answer, so why? We won’t know if we impute the data. From an algorithmic standpoint would be concerned about generating false answers from the data that would in effect define someone for them. For example, if the algorithm was trained off of data that said rich people tend to be white and transgender, if a new rich person had those fields missing, they would be labeled white and transgender as it is the most probable outcome. However, this could very easily not be the case and that person could be mis-identified. Furthermore, they would then be added to a data set which would then reinforce the statistics that lead to the algorithm imputing those fields that way, making future data found with the algorithm more biased and incorrect."
      ],
      "metadata": {
        "id": "7TaicbLBiC7i"
      },
      "id": "7TaicbLBiC7i"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}